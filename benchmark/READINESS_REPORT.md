# 500M readiness gate report

**Generated:** 2026-02-24T12:11:07.659165+00:00

This report was auto-generated by `benchmark/readiness_gate.py`.
It validates whether the ALS-LM model can train on the available hardware
with DeepSpeed ZeRO Stage 2, providing data for a go/no-go decision.

## Hardware

- **GPU:** NVIDIA GeForce RTX 3060 (12.0 GB)
- **CPU:** 5 cores / 10 threads
- **RAM:** 47.0 GB

## Configuration matrix results

Each row represents one benchmark run with the specified model size and settings.

| Config | Params | Offload | Grad Ckpt | Peak GPU (GB) | Tokens/s | Projected Hours (3ep) | Loss@start | Loss@final | CPU RAM (GB) | Disk I/O (R/W MB) | Step Time Var | Status |
|--------|--------|---------|-----------|---------------|----------|-----------------------|------------|------------|--------------|-------------------|---------------|--------|
| 500M_offload-on_gradckpt-on | 537,900,800 | ON | ON | 3.86 | 1,563 | 34.3h | 11.2188 | 7.4961 | 16.2 | 0/4162 | 0.0081 | PASS |
| 500M_offload-on_gradckpt-off | 537,900,800 | ON | OFF | 6.37 | 1,619 | 33.1h | 11.0625 | 7.3242 | 30.3 | 6/6958 | 0.0091 | PASS |
| 500M_offload-off_gradckpt-on | 537,900,800 | OFF | ON | - | - | - | - | - | - | - | - | ERROR: Unable to JIT load the fused_a |
| 500M_offload-off_gradckpt-off | 537,900,800 | OFF | OFF | - | - | - | - | - | - | - | - | ERROR: Unable to JIT load the fused_a |
| 350M_offload-on_gradckpt-on | 304,438,272 | ON | ON | - | - | - | - | - | - | - | - | OOM |

## Checkpoint resume verification

The checkpoint resume test saves training state at step 50, destroys the
model and engine, resets random seeds, rebuilds from scratch, loads the
checkpoint, and verifies loss continuity.

The initial run during the matrix benchmark OOMed due to accumulated GPU memory
from sequential config runs. A re-run on a clean GPU confirmed checkpoint
resume works correctly.

- **Loss at save (step 49):** 9.8125
- **Loss at resume:** 7.6836
- **Ratio (resume/save):** 0.7830
- **Threshold:** resume_loss <= 2.0 * save_loss
- **Result:** PASS


## Projected training duration

Projections are based on the best-performing configuration's throughput,
scaled to production batch settings (batch_size=4, grad_accum=8).

- **Total training tokens:** 128,495,047
- **Tokens per step (production):** 32,768
- **Steps per epoch:** 3,921
- **Benchmark throughput:** 1,619 tokens/sec
- **Estimated production throughput:** 3,237 tokens/sec
- **Scaling note:** Scaled 2.0x from bench batch_size=2 to prod batch_size=4
- **Checkpoint save time:** 5.5s

| Epochs | Total Steps | Training (h) | Checkpoint Overhead (h) | Total (h) | Exceeds 72h |
|--------|-------------|--------------|-------------------------|-----------|-------------|
| 10 | 39,213 | 110.3 | 0.2 | 110.5 | YES |
| 5 | 19,606 | 55.1 | 0.1 | 55.2 | No |
| 3 | 11,764 | 33.1 | 0.1 | 33.1 | No |

## Go/no-go recommendation

**RECOMMENDATION: GO**

The 500M model configuration successfully trained for the benchmark duration
without OOM errors, and checkpoint resume verified loss continuity (ratio 0.78,
well under 2.0 threshold). The chosen configuration is ready for v0.3.0 full
training.

## Chosen configuration

The recommended configuration for v0.3.0 training has been saved to
`configs/500m.json`.

- **Model size:** 500M
- **Parameters:** 537,900,800
- **CPU offload:** Enabled
- **Gradient checkpointing:** Disabled
- **Peak GPU memory:** 6.37 GB
- **Throughput:** 1,619 tokens/sec

---

*Generated by benchmark/readiness_gate.py on 2026-02-24T12:11:07.659165+00:00*
